{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIQ Feature Extraction on Kaggle\n",
    "\n",
    "This notebook runs the feature extraction pipeline for the GIQ benchmark.\n",
    "It assumes the data (renderings and splits) is available as a Kaggle dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if not present\n",
    "!pip install -q timm transformers\n",
    "# mitsuba and open3d are likely not needed for feature extraction, only for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "\n",
    "# Path to the cloned repository (or where the code resides)\n",
    "# If you uploaded the code as a dataset, point to it here.\n",
    "# If you are cloning it in the notebook, use /kaggle/working/repo_name\n",
    "REPO_PATH = Path(\"/kaggle/working/giq-project\")  # Adjust this!\n",
    "\n",
    "# Path to the GIQ data root (containing 'renderings', 'splits', etc.)\n",
    "# On Kaggle, this is usually in /kaggle/input/...\n",
    "DATA_ROOT = Path(\"/kaggle/input/probing-vlm-dataset\") # Adjust this!\n",
    "\n",
    "# Output directory for features\n",
    "OUTPUT_ROOT = Path(\"/kaggle/working/features\")\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------------\n",
    "\n",
    "if not REPO_PATH.exists():\n",
    "    # Clone the repo if it doesn't exist (e.g. if not uploaded as dataset)\n",
    "    # Assuming the user will clone their repo\n",
    "    !git clone https://github.com/phongviet/Probing-the-Geometric-Intelligence-of-Vision-Language-Models.git {REPO_PATH}\n",
    "    # Note: If the repo is private or you have local changes, you might need to upload the code instead.\n",
    "\n",
    "# Add src to python path\n",
    "if str(REPO_PATH) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_PATH))\n",
    "\n",
    "print(f\"Repo path: {REPO_PATH}\")\n",
    "print(f\"Data path: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monkey-patch REPO_ROOT in src.data.base to point to our DATA_ROOT context\n",
    "# The original code assumes data is at REPO_ROOT/data/giq\n",
    "# We want to redirect requests for data to DATA_ROOT.\n",
    "\n",
    "import src.data.base\n",
    "\n",
    "# We can't easily change REPO_ROOT because it's used to construct paths like REPO_ROOT / \"data\" / ...\n",
    "# Instead, we can monkey-patch the functions that use it, or ensure DATA_ROOT has the expected structure.\n",
    "\n",
    "# Let's look at how load_split works:\n",
    "# path = REPO_ROOT / \"data\" / \"giq\" / \"splits\" / f\"{split}_shapes.json\"\n",
    "\n",
    "# If we set REPO_ROOT such that REPO_ROOT / \"data\" / \"giq\" == DATA_ROOT, it works.\n",
    "# So REPO_ROOT should be DATA_ROOT.parent.parent\n",
    "\n",
    "# However, DATA_ROOT in our config points to .../data/giq usually.\n",
    "# So let's try to trick it.\n",
    "\n",
    "class MockPath(type(Path(\".\"))):\n",
    "    def __truediv__(self, other):\n",
    "        # If code asks for REPO_ROOT / \"data\" / \"giq\", redirect to DATA_ROOT\n",
    "        # This is hard to intercept perfectly with Path objects.\n",
    "        return super().__truediv__(other)\n",
    "\n",
    "# Easier approach: Redefine load_split in src.data.base\n",
    "def custom_load_split(split: str):\n",
    "    path = DATA_ROOT / \"splits\" / f\"{split}_shapes.json\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Split file not found: {path}\")\n",
    "    import json\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "src.data.base.load_split = custom_load_split\n",
    "print(\"Patched load_split to use DATA_ROOT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.data.base import GIQBase\n",
    "from src.models.featurizers import (\n",
    "    CLIPFeaturizer,\n",
    "    DINOv3Featurizer,\n",
    "    SigLIP2Featurizer,\n",
    ")\n",
    "\n",
    "# Re-implement ExtractionDataset to accept explicit renderings_root\n",
    "class ExtractionDataset(GIQBase):\n",
    "    \"\"\"\n",
    "    Dataset that iterates over all views of all shapes in a split.\n",
    "    Returns: {\"image\": tensor, \"shape_id\": str, \"view_idx\": int}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split: str,\n",
    "        transform=None,\n",
    "        output_dir: Path | None = None,\n",
    "        limit: int | None = None,\n",
    "        renderings_root: Path | None = None,\n",
    "    ):\n",
    "        super().__init__(split=split, transform=transform, renderings_root=renderings_root)\n",
    "        # We have 20 views per shape (0-19)\n",
    "        self.n_views = 20\n",
    "        self.samples = []\n",
    "\n",
    "        # Filter out already processed shapes if output_dir provided\n",
    "        processed_shapes = set()\n",
    "        if output_dir is not None:\n",
    "            # We assume the output format is {shape_id}.npz\n",
    "            for p in output_dir.glob(\"*.npz\"):\n",
    "                processed_shapes.add(p.stem)\n",
    "\n",
    "        print(f\"Found {len(processed_shapes)} already processed shapes. Skipping them.\")\n",
    "\n",
    "        count = 0\n",
    "        for shape_id in self.shape_ids:\n",
    "            if shape_id in processed_shapes:\n",
    "                continue\n",
    "\n",
    "            # Add all views for this shape\n",
    "            for view_idx in range(self.n_views):\n",
    "                self.samples.append((shape_id, view_idx))\n",
    "\n",
    "            count += 1\n",
    "            if limit is not None and count >= limit:\n",
    "                print(f\"Stopping after {count} shapes due to --limit\")\n",
    "                break\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        shape_id, view_idx = self.samples[idx]\n",
    "        img = self._load_image(shape_id, view_idx)\n",
    "        return {\n",
    "            \"image\": img,\n",
    "            \"shape_id\": shape_id,\n",
    "            \"view_idx\": view_idx,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PARAMETERS ---\n",
    "MODEL_TYPE = \"clip\"   # choices: [\"clip\", \"siglip2\", \"dinov3\"]\n",
    "MODEL_NAME = None     # Optional override\n",
    "SPLIT = \"test\"        # choices: [\"train\", \"val\", \"test\"]\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "LIMIT = None          # Set to integer to test on few shapes\n",
    "FP16 = True\n",
    "\n",
    "# ------------------\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if MODEL_TYPE == \"clip\":\n",
    "    model_name = MODEL_NAME or \"openai/clip-vit-base-patch16\"\n",
    "    featurizer = CLIPFeaturizer(model_name=model_name, device=device)\n",
    "elif MODEL_TYPE == \"siglip2\":\n",
    "    model_name = MODEL_NAME or \"google/siglip2-base-patch16-224\"\n",
    "    featurizer = SigLIP2Featurizer(model_name=model_name, device=device)\n",
    "elif MODEL_TYPE == \"dinov3\":\n",
    "    model_name = MODEL_NAME or \"facebook/dinov3-base\"\n",
    "    featurizer = DINOv3Featurizer(model_name=model_name, device=device)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model: {MODEL_TYPE}\")\n",
    "\n",
    "model_safe_name = model_name.replace(\"/\", \"__\")\n",
    "output_dir = OUTPUT_ROOT / model_safe_name / SPLIT\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving features to: {output_dir}\")\n",
    "\n",
    "renderings_path = DATA_ROOT / \"renderings\"\n",
    "\n",
    "transform = featurizer.get_transform()\n",
    "dataset = ExtractionDataset(\n",
    "    split=SPLIT, \n",
    "    transform=transform, \n",
    "    output_dir=output_dir, \n",
    "    limit=LIMIT,\n",
    "    renderings_root=renderings_path\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Extracting features for {len(dataset)} images ({len(dataset.shape_ids)} shapes x 20 views)...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Extraction\n",
    "\n",
    "shape_buffers = {}  # shape_id -> {'global': [], 'local': [], 'views': []}\n",
    "\n",
    "for batch in tqdm(loader):\n",
    "    images = batch[\"image\"].to(device)\n",
    "    shape_ids = batch[\"shape_id\"]\n",
    "    view_idxs = batch[\"view_idx\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if FP16 and \"cuda\" in device:\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                features = featurizer(images)\n",
    "        else:\n",
    "            features = featurizer(images)\n",
    "\n",
    "    # Move to CPU\n",
    "    global_feats = features[\"global\"].cpu().numpy()  # [B, D]\n",
    "    local_feats = features[\"local\"].cpu().numpy()  # [B, N, D]\n",
    "\n",
    "    for i, sid in enumerate(shape_ids):\n",
    "        vid = int(view_idxs[i])\n",
    "\n",
    "        save_path = output_dir / f\"{sid}.npz\"\n",
    "        if save_path.exists():\n",
    "            continue\n",
    "\n",
    "        if sid not in shape_buffers:\n",
    "            shape_buffers[sid] = {\"global\": [], \"local\": [], \"views\": []}\n",
    "\n",
    "        shape_buffers[sid][\"global\"].append(global_feats[i])\n",
    "        shape_buffers[sid][\"local\"].append(local_feats[i])\n",
    "        shape_buffers[sid][\"views\"].append(vid)\n",
    "\n",
    "        # Check if we have all 20 views\n",
    "        if len(shape_buffers[sid][\"views\"]) == 20:\n",
    "            # Sort by view index\n",
    "            indices = np.argsort(shape_buffers[sid][\"views\"])\n",
    "            g_sorted = np.stack(shape_buffers[sid][\"global\"])[indices]\n",
    "            l_sorted = np.stack(shape_buffers[sid][\"local\"])[indices]\n",
    "\n",
    "            # Save\n",
    "            np.savez_compressed(\n",
    "                save_path, global_features=g_sorted, local_features=l_sorted\n",
    "            )\n",
    "\n",
    "            # Clear buffer\n",
    "            del shape_buffers[sid]\n",
    "\n",
    "# Flush any remaining\n",
    "for sid, buf in shape_buffers.items():\n",
    "    if len(buf[\"views\"]) > 0:\n",
    "        print(f\"Warning: Incomplete views for {sid} (found {len(buf['views'])}). Saving anyway.\")\n",
    "        indices = np.argsort(buf[\"views\"])\n",
    "        g_sorted = np.stack(buf[\"global\"])[indices]\n",
    "        l_sorted = np.stack(buf[\"local\"])[indices]\n",
    "        np.savez_compressed(\n",
    "            output_dir / f\"{sid}.npz\",\n",
    "            global_features=g_sorted,\n",
    "            local_features=l_sorted,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate output\n",
    "print(f\"Processed {len(list(output_dir.glob('*.npz')))} files.\")\n",
    "if len(list(output_dir.glob('*.npz'))) > 0:\n",
    "    sample_file = list(output_dir.glob('*.npz'))[0]\n",
    "    data = np.load(sample_file)\n",
    "    print(f\"Sample {sample_file.name}: Global {data['global_features'].shape}, Local {data['local_features'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
